{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count with hdfs3, distributed, and dask\n",
    "\n",
    "In this example, we count the number of words in text files (Enron email dataset - 6.4 GB) stored in HDFS.\n",
    "\n",
    "We use the libraries in an increasing order of API functionality, from low-level to high-level:\n",
    "\n",
    "* hdfs3\n",
    "* hdfs3 + distributed\n",
    "* hdfs3 + distributed + dask\n",
    "\n",
    "Setup:\n",
    "\n",
    "Copy data from S3 into HDFS:\n",
    "\n",
    "```\n",
    "$ hadoop distcp \n",
    "```\n",
    "\n",
    "Install dependencies\n",
    "\n",
    "```\n",
    "$ conda install hdfs3 -c blaze\n",
    "$ pip install dask distributed\n",
    "```\n",
    "\n",
    "TODO: implement proper count by word instead of total word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1) Word count with hdfs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hdfs3\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdfs = hdfs3.HDFileSystem('ip-172-31-56-96', port=8020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate list of foldernames and filenames in /tmp/enron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = hdfs.glob('/tmp/enron/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'/tmp/enron/edrm-enron-v2_shapiro-r_xml.zip/merged.txt',\n",
       " b'/tmp/enron/edrm-enron-v2_skilling-j_xml.zip/merged.txt',\n",
       " b'/tmp/enron/edrm-enron-v2_mclaughlin-e_xml.zip/merged.txt',\n",
       " b'/tmp/enron/edrm-enron-v2_germany-c_xml.zip/merged.txt',\n",
       " b'/tmp/enron/edrm-enron-v2_cash-m_xml.zip/merged.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print first 500 bytes of first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Date: Sat, 7 Jul 2001 07:20:00 -0700 (PDT)\\r\\nFrom: Jo Ann Hill\\r\\nTo: Janine Migden\\r\\nCc: Richard Shapiro\\r\\nSubject: PRC Ratings - Non-Exempt (Migden)\\r\\nX-SDOC: 738886\\r\\nX-ZLID: zl-edrm-enron-v2-shapiro-r-9869.eml\\r\\n\\r\\nJanine -- \\r\\n\\r\\nEarlier this week, you received an email from the PEP team which included the \\r\\nPerformance Evaluations of your employees, both Exempt and Non-Exempt.  \\r\\nPlease remember that the final Performance Evaluations should be completed \\r\\nwith your employees, forms signed and returned to me at EB1665B no later than \\r\\nFriday, August 17th.  I will be preparing a report for your use noting the \\r\\nfinal ratings of your Exempt employees from the PRC meetings and distribute \\r\\nthat within the next week.\\r\\n\\r\\nFor your Non-Exempt employees, I have listed below all eligible employees for \\r\\nwhom you should complete a Performance Evaluation.  However, I need the \\r\\nrating for each employee so it may be input into the PEP system.  The \\r\\ndescriptions for Non-Exempt ratings are defined as:\\r\\n\\r\\n1 - Consistently perfor'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdfs.head(filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words(filename):\n",
    "    word_counts = defaultdict(int)\n",
    "    with hdfs.open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                word_counts[word] += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.73 s, sys: 90.4 ms, total: 5.82 s\n",
      "Wall time: 5.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts_single = count_words(filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'the', 361079),\n",
       " (b'to', 228363),\n",
       " (b'of', 206534),\n",
       " (b'and', 182790),\n",
       " (b'in', 126461),\n",
       " (b'a', 114219),\n",
       " (b'for', 82769),\n",
       " (b'is', 76657),\n",
       " (b'that', 74224),\n",
       " (b'by', 54229)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_counts_single.items(), key=lambda k_v: k_v[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in all (readable) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 456 ms, total: 22.7 s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wordcounts = {}\n",
    "for filename in filenames[:5]:\n",
    "    wordcounts[filename] = count_words(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict_args):\n",
    "    '''\n",
    "    Given any number of dicts, shallow copy and merge into a new dict,\n",
    "    precedence goes to key value pairs in latter dicts.\n",
    "    '''\n",
    "    result = {}\n",
    "    for dictionary in dict_args:\n",
    "        result.update(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_dicts = [x for x in wordcounts.values()]\n",
    "merged_dicts = merge_dicts(all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748877"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'0', 4734110),\n",
       " (b'-', 126643),\n",
       " (b'Phy', 96008),\n",
       " (b'TAGG/', 89524),\n",
       " (b'01-09-2001', 85333),\n",
       " (b'I', 82468),\n",
       " (b'JARNOLD', 81540),\n",
       " (b'TCO', 77891),\n",
       " (b'&', 75190),\n",
       " (b'to', 71976)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(merged_dicts.items(), key=lambda k_v: k_v[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes and missing API functionality?\n",
    "\n",
    "* ~~wanted to list only filenames (and not other HDFS file info)~~\n",
    "* ~~wanted to load all text files in subdirs (glob like /tmp/enron/*/*.txt)~~\n",
    "* ~~wanted to set encoding in .open() method~~\n",
    "* ~~wanted to easily read .head() of large text file~~\n",
    "* ~~why are the encoding errors happening?~~\n",
    "* why are some word counts zero?\n",
    "* glob returns unordered list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2) Word count with hdfs3 + distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start dscheduler and dworkers on nodes:\n",
    "\n",
    "head node: `dscheduler`  \n",
    "compute nodes: `dworker 172.31.56.96:8786`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from distributed import Executor, progress, wait\n",
    "from distributed.hdfs import read_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = Executor('172.31.56.96:8786')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "future = e.submit(count_words, filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in all (readable) files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "futures = e.map(count_words, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "futures[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "wait(futures);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "futures[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "futures[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(e.gather(futures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes and missing API functionality?\n",
    "\n",
    "* ~~how to view number of workers/cores?~~\n",
    "* ~~wanted to easily read .head() of large text file~~\n",
    "* ~~can I pass arguments to functions in e.map(func, input)?~~\n",
    "* ~~can I get the results of futures without a list comprehension, similar to an RDD?~~\n",
    "* ~~why are the encoding errors happnening?~~\n",
    "* ~~should blocking futures be default with an option to wait()?~~\n",
    "* ~~when to use readbytes, BytesIO, etc.~~\n",
    "* ~~are these errors from dscheduler important? https://gist.github.com/koverholt/6c0f9c10b23152c3f0c4~~\n",
    "* option to drop failed futures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3) Word count with hdfs3 + distributed + dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from distributed.hdfs import read_bytes\n",
    "from distributed.collections import futures_to_collection, futures_to_dask_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bytes = read_bytes('/tmp/enron/*/*', hdfs=hdfs, delimiter=b'\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bytes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bytes_to_lines(b):\n",
    "    return b.decode().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lists = e.map(bytes_to_lines, bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "db = futures_to_dask_bag(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words_in_bytes(data):\n",
    "    words = data.split()\n",
    "    count = len(words)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_counts = db.map(count_words_in_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "word_counts.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes and missing API functionality?\n",
    "\n",
    "* readbytes reads everything into memory vs. lazy execution?\n",
    "* wanted to easily read .head() of large text file\n",
    "* different word count than hdfs3 + distributed, perhaps due to line splitting\n",
    "* solution was a bit more complex than hdfs3 + distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future.exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark API\n",
    "\n",
    "```\n",
    ">>> rdd = sc.textFile('/tmp/enron/*/*.txt')\n",
    ">>> counts = rdd.flatMap(lambda line: line.split()).count()\n",
    "...\n",
    "16/02/02 07:01:47 INFO DAGScheduler: ResultStage 0 (count at <stdin>:1) finished in 76.110 s\n",
    "16/02/02 07:01:47 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool\n",
    "16/02/02 07:01:47 INFO DAGScheduler: Job 0 finished: count at <stdin>:1, took 76.307768 s\n",
    ">>> counts\n",
    "913806131\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Time to count words in all files:\n",
    "\n",
    "* hdfs3 - 4 min 8 s\n",
    "* hdfs3 + distributed - 1 min 30 s\n",
    "* hdfs3 + distributed + dask - 2 min 2 s\n",
    "* PySpark - 1 min 16 s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
